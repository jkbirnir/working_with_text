<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>working_with_text</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="working_with_text1_files/libs/clipboard/clipboard.min.js"></script>
<script src="working_with_text1_files/libs/quarto-html/quarto.js"></script>
<script src="working_with_text1_files/libs/quarto-html/popper.min.js"></script>
<script src="working_with_text1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="working_with_text1_files/libs/quarto-html/anchor.min.js"></script>
<link href="working_with_text1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="working_with_text1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="working_with_text1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="working_with_text1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="working_with_text1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="working_with_text1_files/libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<link href="working_with_text1_files/libs/wordcloud2-0.0.1/wordcloud.css" rel="stylesheet">
<script src="working_with_text1_files/libs/wordcloud2-0.0.1/wordcloud2-all.js"></script>
<script src="working_with_text1_files/libs/wordcloud2-0.0.1/hover.js"></script>
<script src="working_with_text1_files/libs/wordcloud2-binding-0.2.1/wordcloud2.js"></script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">working_with_text</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>First, install the packages you will likely need to conduct the text analysis.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:stats':

    filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidytext)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="text-analysis" class="level1">
<h1>Text analysis</h1>
<p>In this tutorial we are going to start working with text. We will take the tidy approach to text analysis because it is fairly intuitive in that it treats text as data frames for analysis. Furthermore, the tibble data frames created in tidy are small and don’t require as much processing power.</p>
<p>The text converted into a data frame can be converted to a variety of formats that allow it to be analyzed by other popular r programs such as quanteda. While the examples are different and we won’t cover all of the topics in the book you can find much of this material in <a href="https://www.tidytextmining.com/index.html" title="A Tidy Approach"><strong>Text Mining with R</strong></a><strong>: A Tidy Approach</strong> listed in the syllabus</p>
<p>This image from <a href="https://www.tidytextmining.com/index.html" title="A Tidy Approach"><strong>Text Mining with R</strong></a><strong>: A Tidy Approach</strong> demonstrates the general overall process for text analysis using tidyverse:</p>
<p><img src="Images/2.png" class="img-fluid"></p>
<section id="a-corpus" class="level2">
<h2 class="anchored" data-anchor-id="a-corpus">A Corpus</h2>
<p>For our analysis we need text. The raw text is the corpus that we work on. The corpus that we are interested in varies depending on our research objectives. For example, a corpus can contain a newspaper articles, multiple articles, paragraphs from articles, tweets about a particular topic or from a group of people, text from pdf documents that we have collected etc. How we put together the relevant corpus depends on the objective of our analysis.</p>
<p>Today we will work with some data that has already been provided to us. The first is text from newspaper articles that were previously scraped and the paragraphs collected into a dataframe.</p>
<p>The first step is to load the data. It was saved as an R Object, so we can load it the following way:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"multiple_url_paragraphs.RData"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This loads the R data object into our working environment to allow us to begin working with the data. As you can see, it’s loaded as, multi_df.</p>
<p>The multi_df data consists of a number of newspaper paragraphs collected from the web and an id that identifies the number of the paragraph in a given article. The id numbers repeat for each article-paragraph.</p>
<p><img src="Images/7.png" class="img-fluid"></p>
</section>
</section>
<section id="tokenization" class="level1">
<h1>Tokenization</h1>
<p>To work with the unmodified corpus data we need to render it into a format that we can use for text analysis. Specifically, the data we need to turn it into a tokenized cleaned dataframe.</p>
<p>For this tutorial, we will be using the tidyverse text analysis method because it is very intuitive so begin by loading the tidytext package and installing it if not installed yet. This will be one of the core packages needed for our work.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("tidytext")</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidytext)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can begin to work with the data.</p>
<p>First we want to transform the corpus into something more usable usable by tokenizing the text.</p>
<p>By tokenizing the corpus we are separating the paragraphs and sentences into words instead of the format they are currently in.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>tidy_paragraphs <span class="ot">&lt;-</span> multi_df <span class="sc">%&gt;%</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we look at tidy_paragraphs, we can see that the text is broken down by word and noted by the paragraph it is found in. The paragraph id repeats for each article-paragraph-word that we downloaded that contains text.</p>
<p><img src="Images/1.png" class="img-fluid"></p>
<section id="cleaning" class="level3">
<h3 class="anchored" data-anchor-id="cleaning">Cleaning</h3>
<p>Now we want to begin cleaning the corpus by removing unnecessary words. These are words that convey no meaningful information but appear frequently, usually referred to as “stop words” such as but, and, if, then, and similar words.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>tidy_paragraphs <span class="ot">&lt;-</span> tidy_paragraphs <span class="sc">%&gt;%</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anti_join</span>(stop_words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Joining with `by = join_by(word)`</code></pre>
</div>
</div>
<p>This uses the “stop words” dataset from tidyverse to remove the most common stop words.</p>
<p>We can then look at the top words in our articles using the following function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>tidy_paragraphs <span class="sc">%&gt;%</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(word, <span class="at">sort =</span> <span class="cn">TRUE</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This shows that the top words in these articles, with stop words removed, are Munich, western, Ukraine, war, countries, and Germany.</p>
<p>You can also clean words/spaces/numbers that aren’t stop words. This can be especially useful if you accidentally include advertisements or other unnecessary data in your scraping.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>tidy_paragraphs <span class="ot">&lt;-</span>tidy_paragraphs <span class="sc">%&gt;%</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span>word <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"Politico"</span>,<span class="st">"reporting"</span>)) <span class="co">#Add in the word you'd like to remove here to filter them out</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can also clean out any numbers with the following code.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>tidy_paragraphs <span class="ot">&lt;-</span>tidy_paragraphs <span class="sc">%&gt;%</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>          <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">grepl</span>(<span class="st">"[0-9]+"</span>, word)) </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">#filters out non-grepl (string) digits from 0-9 in any form in the word column</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Cleaning is a lengthy and involved process that differs between every project and depends on the data you are using. Take your time cleaning the data to get the best results.</p>
</section>
</section>
<section id="analyzing-the-data" class="level1">
<h1>Analyzing the Data</h1>
<p>Next we want to analyze the data.</p>
<section id="wordcloud-visualization" class="level2">
<h2 class="anchored" data-anchor-id="wordcloud-visualization">Wordcloud Visualization</h2>
<p>One common way to analyze the data is to visualize them.</p>
<p>For example, the most commonly used words can be visualized using a word cloud. Two packages, devtools and wordcloud2, are needed to do this.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(wordcloud2)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(devtools)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: usethis</code></pre>
</div>
</div>
<p>A wordcloud displays the words in the corpus by size according to how often the words occur in the corpus. To show this we first must count the words.</p>
<p>Words that occur more frequently will be larger and the ones that occur less frequently will be smaller.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>t_para <span class="ot">&lt;-</span> tidy_paragraphs <span class="sc">%&gt;%</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(word, <span class="at">sort =</span> <span class="cn">TRUE</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the new object (t_para) counts words in the entire tokenized corpus we fed to it (tidy_paragraphs) and did NOT treat each paragraph/article as a separate entity. You can make a very simple wordcloud using the wordcloud2() function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">wordcloud2</span>(t_para)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div class="wordcloud2 html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-8f76c4c88959bc3661c7" style="width:100%;height:464px;"></div>
<script type="application/json" data-for="htmlwidget-8f76c4c88959bc3661c7">{"x":{"word":["munich","western","ukraine","war","countries","germany","central","europe","president","tanks","zelenskyy","european","russian","scholz","security","support","ukraine’s","allies","defense","german","policy","chancellor","conference","europeans","history","leopard","military","ukrainian","account","alternative","berlin","country","goliath","historical","intelligence","lose","morawiecki","nato","partners","putin","russia","russia’s","there’s","u.s","acceptance","access","actionable","alliance","confirm","conflict","content","data","depth","forgot","invasion","join","kyiv","leaders","log","logging","manage","opportunity","password","politico","press","privacy","professionals","profile","register","reporters","send","sending","speed","tank","vladimir","warned","west","added","alliances","austin","baltics","battle","begun","belarus","belarusian","biblical","boot","calling","commit","compared","conferences","counterparts","crucial","cycles","david","defeat","deliver","donations","drawn","east","economic","election","enthusiastic","eu","event","fear","fight","friday","gathering","global","including","informed","international","involved","kuisz","minister","modern","neighbors","officials","past","percent","pistorius","poles","political","process","recent","remarks","responsibility","shared","sling","soviet","spending","start","strength","stretching","they’ve","told","training","trap","union","unity","urging","volodymyr","win","world","acceleration","acknowledges","adding","adhere","administration","admit","adversaries","ages","ago","agreed","agreement","ahead","air","alarm","alexander","align","annual","anxiety","anxious","appeasement","appetite","applause","arbitrarily","aren’t","article","artificial","artillery","asleep","attacked","audience","barely","based","battlefield","betrayal","betting","bid","biden’s","billion","boost","boris","bound","bring","britain’s","bucha","build","built","bulgaria","bullying","buts","cajoling","call","campaign","can’t","cautious","centuries","challenge","chamberlain","changing","chiefs","churning","closed","cohesion","cohort","commissars","commitment","compassion","conclusion","concretely","congress","considerable","contact","contrast","convince","counting","country’s","cultural","czars","czechs","daladier","danger","date","day","days","decision","definitive","delivering","desk","determined","devastatingly","diplomacy","direction","dismissed","display","don’t","drag","dragging","drive","driven","dubious","duty","earnest","ease","easier","eastern","easy","editor","education","embrace","empathy","ensure","entitled","entrapped","escaped","established","estonia","eventually","exception","exit","expect","experience","explain","expressly","extended","fall","families","feet","fighting","finally","finland","fleeing","fleet","focus","focused","fold","follow","forces","forums","founded","founder","france’s","freedom","friends","frozen","fulfill","fund","future","gains","gathered","gaze","generations","genetic","germany’s","giant","giving","goal","government","grab","green","grinded","growing","haul","haunt","hesitated","he’s","highlight","historian","histories","hoped","hopes","horrors","hours","hungary","identities","ifs","impelling","industry","inextricably","infamous","influential","informing","innately","insurance","integration","intensively","interactions","interplay","interpreted","invaded","investment","iran","irpin","israelite","jarosław","joe","joining","joseph","kilometer","kitchen","kultura","leadership","leader’s","leave","led","left","lens","level","liberalna","lie","linguistic","linking","listen","lloyd","location","logistical","losses","low","lukashenko","lurch","maintains","massive","mateusz","meeting","memorialize","mention","message","miscalculation","miss","mnichovská","month","months","moral","motives","nation","negotiated","negotiations","nervousness","neville","niggling","notable","notably","noted","observers","offer","offering","olaf","olive","opposed","organize","output","partial","partnerships","passed","people","perception","permanent","permanently","perspective","philistine","play","pledge","pledges","podcast","poland","poland’s","policymakers","polish","portugal","post","potsdam","praise","predicted","preemptive","prepare","prepared","pressing","previous","primarily","prime","private","probability","proceed","production","prompted","provide","proximity","psychologists","public","pulling","purposefulness","putin’s","question","quickly","rally","ramstein","rattling","reacted","reasons","received","refugees","region","regroup","reportedly","represented","republican","reset","reshaping","response","saber","save","scandinavia","scholz’s","secretary","sense","series","set","settlement","shaping","shells","shepherd","shoulder","shown","shudder","signed","significant","sitting","size","soldiers","spain","speaking","special","speech","spilling","spoke","spurning","stalin","starts","stocks","stomach","stone","stop","story","stranglehold","strategies","strong","stronger","struggling","sudetenland","suffer","superseded","supply","supporting","survival","sweatshirt","switch","systems","tables","takes","talk","talks","target","targets","tasks","thanked","thursday","ties","time","times","tire","total","trademark","transatlantic","treatment","trepidation","trimming","troops","tuesday","ukrainians","uncompromised","underlying","underpinned","underscoring","understanding","uninvited","unshakable","urges","vanquished","version","victory","visegrád","voiced","vowed","wall","warplanes","weapons","weary","week","weekly","wide","wise","woken","won’t","worries","write","yalta","zrada","édouard"],"freq":[14,13,12,11,10,10,9,8,8,8,8,7,7,7,7,7,7,6,6,6,6,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"fontFamily":"Segoe UI","fontWeight":"bold","color":"random-dark","minSize":0,"weightFactor":12.8571428571429,"backgroundColor":"white","gridSize":0,"minRotation":-0.785398163397448,"maxRotation":0.785398163397448,"shuffle":true,"rotateRatio":0.4,"shape":"circle","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":{"render":[{"code":"function(el,x){\n                        console.log(123);\n                        if(!iii){\n                          window.location.reload();\n                          iii = False;\n\n                        }\n  }","data":null}]}}</script>
</div>
</div>
<p>This then outputs a simple wordcloud of the most commonly use words in our articles.</p>
</section>
<section id="word-sequences-n-grams" class="level2">
<h2 class="anchored" data-anchor-id="word-sequences-n-grams">Word sequences N-grams</h2>
<p>Alternatively we may want to establish relationships between words. For example, by seeing how often word X is followed by word Y, we can build a model of the relationships between them.</p>
<p>To do this we go back to the un-tokenized corpus multi_df.</p>
<p>We can then use the function unnest_tokens as used earlier in order to tokenize into consecutive sequences of words (as opposed to single words), called n-grams.</p>
<p>We do this by adding the token = “ngrams” option to unnest_tokens(), and setting n to the number of words we wish to capture in each n-gram. When we set n to 2, we are examining pairs of two consecutive words, often called “bigrams”:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>tidy_bigrams <span class="ot">&lt;-</span> multi_df <span class="sc">%&gt;%</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(bigram, text, <span class="at">token =</span> <span class="st">"ngrams"</span>, <span class="at">n =</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(bigram)) <span class="co">#NOTE here we are filtering out any words that are not a part of a bigram</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our usual tidy tools apply equally well to n-gram analysis. We can examine the most common bigrams using dplyr’s count():</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>tidy_bigrams <span class="sc">%&gt;%</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(bigram, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It then shows the pairs of words that frequently appear together. Looking at it, you can see that the words are often two stop words and thus, uninteresting. We can clean the data in order to filter the stop words and focus on the interesting text.</p>
<p>This is a useful time to use tidyr’s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases (rows) where either is a stop-word.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>bigrams_separated <span class="ot">&lt;-</span> tidy_bigrams <span class="sc">%&gt;%</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(bigram, <span class="fu">c</span>(<span class="st">"word1"</span>, <span class="st">"word2"</span>), <span class="at">sep =</span> <span class="st">" "</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>bigrams_filtered <span class="ot">&lt;-</span> bigrams_separated <span class="sc">%&gt;%</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span>word1 <span class="sc">%in%</span> stop_words<span class="sc">$</span>word) <span class="sc">%&gt;%</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span>word2 <span class="sc">%in%</span> stop_words<span class="sc">$</span>word)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># new bigram counts:</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>bigram_counts <span class="ot">&lt;-</span> bigrams_filtered <span class="sc">%&gt;%</span> </span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(word1, word2, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can see the more interesting results:</p>
<p><img src="Images/4.png" class="img-fluid"></p>
</section>
<section id="sentiment-analysis" class="level2">
<h2 class="anchored" data-anchor-id="sentiment-analysis">Sentiment Analysis</h2>
<p>When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically, as shown below:</p>
<p><img src="Images/5.png" class="img-fluid"></p>
<p>There already exist quite a few existing dictionaries and methods for sentiment analysis that make our work easier. The tidytext package provides access to several ready made sentiment lexicons.</p>
<p>All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, or sadness.</p>
<p>The function get_sentiments() allows us to get specific sentiment lexicons with the appropriate measures for each one.</p>
<p>The following code is an example of how to use an existing sentiment dictionary to analyze the text we worked with earlier. In order to make the analysis more interesting, we can separate the analysis by document. To do this, we will use data scraped from some pdf files.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"pdf_frame1.RData"</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>pdf_frame1<span class="ot">=</span>pdf_frame1<span class="sc">%&gt;%</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="st">"document"</span>, <span class="st">"term"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Looking at this dataframe you see that it contains 2 column 1 for the document number, one for the document-word.</p>
<p>First, rename the “term” column in pdf_frame1 to word so it’s easier to join with the sentiment dictionary.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(pdf_frame1)[<span class="dv">2</span>] <span class="ot">=</span><span class="st">"word"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, run this code to join the sentiment dictionary with the scraped text and derive a sentiment score. This uses the “bing” sentiment dictionary, which comes with the tidyverse packages:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>multi_doc_sentiment <span class="ot">&lt;-</span> pdf_frame1 <span class="sc">%&gt;%</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(<span class="fu">get_sentiments</span>(<span class="st">"bing"</span>), <span class="at">by=</span><span class="st">"word"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(document, sentiment) <span class="sc">%&gt;%</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> sentiment, <span class="at">values_from =</span> n, <span class="at">values_fill =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sentiment =</span> positive <span class="sc">-</span> negative)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note count(document, sentiment) asks the program to count sentiment words by document and not in the whole corpus. This is useful for comparing one document to the next.</p>
<p>Then the overall sentiment of the document is counted (positive vs.&nbsp;negative words)</p>
<p>Now we can compare the overall sentiments between documents data in the , just as an example of what you can do with sentiment analysis. This can be done via ggplot2.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(multi_doc_sentiment, <span class="fu">aes</span>(document, sentiment, <span class="at">fill =</span> document)) <span class="sc">+</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>document, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">scales =</span> <span class="st">"free_x"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>p2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="working_with_text1_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>As shown in these graphs, document 4 is significantly more negative in sentiment than the other three documents.</p>
<p>This is not altogether surprising as the first 3 documents are judicial opinions whereas the 4th document is a Security council statement on the situation in the Yugoslav republic.</p>
</section>
</section>
<section id="other-packages-and-options" class="level1">
<h1>Other packages and options</h1>
<p>Text analysis has gained significant steam and multiple packages have been developed to analyze sentiment and other features of text. Some of the interesting and popular packages out there include: <a href="http://quanteda.io/">Quanteda</a>, <a href="https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf">Tm</a> and many others. Some, including quanteda are discussed in the book we based this tutorial largely on <a href="https://www.tidytextmining.com/index.html" title="A Tidy Approach"><strong>Text Mining with R</strong></a>, some are general purpose others satisfy a specific requirement. We encourage you to read more if you are interested in exploring this further.</p>
<p>Furthermore, when you develop your own research you may want to develop your own dictionary for text depending on what you are looking for. For example, the AMAR project (see ilcss) developed dictionaries to detect ethnic minority groups and a variety of activity they engage in in news paper text. We will learn more about this March 7th.</p>
</section>
<section id="assignment" class="level1">
<h1>Assignment</h1>
<p>Pull this tutorial and all associated documents onto your computer from github (https://github.com/jkbirnir/working_with_text). Use the news data provided on github (or another corpus of your choosing) to tokenize this corpus and clean of stopwords to produce a graphical representation (word frequency, wordcloud etc.). Upload to ELMS.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>