---
title: "working_with_text"
format: html
editor: visual
---

First, install the packages you will likely need to conduct the text analysis.

```{r}
library(dplyr)
library(tidytext)
library(stringr)
```

# Text analysis

In this tutorial we are going to start working with the text that we scraped last time. We will take the tidy approach to text analysis because it is fairly intuitive in that it treats text as data frames for analysis.

Furthermore, the text converted into a data frame can be converted to a variety of formats that allow it to be analyzed by other popular ra programs such as quanteda. While the examples are different and we won't cover all of the topics in the book you can find much of this material in [**Text Mining with R**](https://www.tidytextmining.com/index.html "A Tidy Approach")**: A Tidy Approach** listed in the syllabus

This image from [**Text Mining with R**](https://www.tidytextmining.com/index.html "A Tidy Approach")**: A Tidy Approach** demonstrates the general overall process for text analysis using tidyverse:

![](Images/2.png)

## A Corpus

For our analysis we need text. The raw text is the corpus that we work on. The corpus that we are interested in varies depending on our research objectives. For example, a corpus can contain a newspaper article, multiple articles, tweets about a particular topic or from a group of people. How we put together the relevant corpus depends on the objective of our analysis.

## Working with pdf files

For further information see:

https://data.library.virginia.edu/reading-pdf-files-into-r-for-text-mining/

#### 1: Package for reading pdf

```{r}
install.packages("pdftools")
library(pdftools)
```

#### 2:

Create a vector of pdf file names from the pdf files in your directory (remember to set the working directory to the directory where you are keeping your files)

```{r}
files <- list.files(pattern = "pdf$")
```

#### 3: Extract the text from the pdf

```{r}
documents <- lapply(files, pdf_text)

```

Verify how many documents were read:

```{r, echo=FALSE}
length(documents)
```

The number of pages in each document

```{r, echo=FALSE}
lapply(documents, length) 
```

To see the documents

```{r, echo=FALSE}
documents
```

#### 4: Convert to document-term-matrix

```{r}
library(tidyverse)
library(tidytext)
library(pdftools)
library(tm)
library(broom)

converted <- Corpus(VectorSource(documents)) %>%
          DocumentTermMatrix()
```

#### 5: Convert to a tidy format

```{r}
converted1 =converted %>%
          tidy() %>%
          filter(!grepl("[0-9]+", term)) #filters out nongrepl (string) digits
```

# Cleaning

As an example, here we will use the newspaper article that we scraped.

The first step is to load the data that we saved in the last project. It was saved as an R Object, so we can load it the following way:

```{r}
load("multiple_url_paragraphs.RData")
```

This loads the saved R data object into our working environment to allow us to begin working with the data. As you can see, it's loaded as what we saved the object as previously, multi_df.

## Cleaning the Data

We now need to work on cleaning the data to render it into a format that we can use for text analysis. For this tutorial, we will be using the tidyverse text analysis method so begin by loading the tidytext package and installing it if not installed yet. This will be one of the core packages needed for our work.

```{r}
# install.packages("tidytext")
library(tidytext)
```

Now we can begin to work with the data and transform it into something usable.

Begin by tokenizing the data, or separating the paragraphs and sentences into words instead of the format they are currently in.

```{r}
tidy_paragraphs <- multi_df %>%
  unnest_tokens(word, text)
```

If we look at tidy_paragraphs, we can see that the text is broken down by word and noted by the paragraph it is found in:

![](Images/1.png)

Now we want to begin cleaning the corpus by removing unnecessary words. These are words that convey no meaningful information but appear frequently, usually referred to as "stop words" such as but, and, if, then, and similar words.

```{r}
tidy_paragraphs <- tidy_paragraphs %>%
  anti_join(stop_words)
```

This uses the "stop words" dataset from tidyverse to remove the most common stop words.

We can then look at the top words using the following function:

```{r, eval=FALSE}
tidy_paragraphs %>%
  count(word, sort = TRUE) 
```

This shows that the top words in these articles, with stop words removed, are Munich, western, Ukraine, war, countries, and Germany.

You can also clean words that aren't stop words. This can be especially useful if you accidentally include advertisements or other unnecessary data in your scraping.

```{r}
tidy_paragraphs <-tidy_paragraphs %>%
  filter(!word %in% c("Politico","reporting")) #Add in the word you'd like to remove here to filter them out
```

# Visualization

The most commonly used words can be visualized using a word cloud. Two packages, devtools and wordcloud2, are needed to do this.

```{r}
library(wordcloud2)
library(devtools)
```

First, we must figure out the word counts:

```{r}
t_para <- tidy_paragraphs %>%
  count(word, sort = TRUE) 
```

You can make a very simple wordcloud using the wordcloud2() function.

```{r}
wordcloud2(t_para)
```

This then outputs a simple wordcloud of the most commonly use words in our articles.

# Analyzing the Data

Now we can look at other ways to analyze the text data we have scraped.

## N-grams

We can use the function unnest_tokens as used earlier in order to tokenize into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.

We do this by adding the token = "ngrams" option to unnest_tokens(), and setting n to the number of words we wish to capture in each n-gram. When we set n to 2, we are examining pairs of two consecutive words, often called “bigrams”:

```{r}
tidy_bigrams <- multi_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))
```

Our usual tidy tools apply equally well to n-gram analysis. We can examine the most common bigrams using dplyr’s count():

```{r, eval=FALSE}
tidy_bigrams %>%
  count(bigram, sort = TRUE)
```

It then shows the pairs of words that frequently appear together. Looking at it, you can see that the words are often two stop words and thus, uninteresting. We can clean the data in order to filter the stop words and focus on the interesting text.

This is a useful time to use tidyr’s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word.

```{r}
library(tidyr)

bigrams_separated <- tidy_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
```

Now we can see the more interesting results:

![](Images/4.png)

## Sentiment Analysis

When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically, as shown below:

![](Images/5.png)

There are quite a few existing dictionaries and methods for sentiment analysis that make our work easier. The tidytext package provides access to several sentiment lexicons. 


All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, or sadness.

The function get_sentiments() allows us to get specific sentiment lexicons with the appropriate measures for each one.

The following code is an example of how to use a sentiment dictionary to analyze the paragraphs we scraped earlier. In order to make the analysis interesting, we can separate the analysis by document. To do this, we will use the pdf files scraped earlier.

First, rename the "term" column in converted1 to word so it's easier to join with the sentiment dictionary.

```{r}
colnames(converted1)[2] ="word"
```

Now, run this code to join the sentiment dictionary with the scraped text and derive a sentiment score. This uses the "bing" sentiment dictionary, which comes with the tidyverse packages:

```{r}
multi_doc_sentiment <- converted1 %>%
  inner_join(get_sentiments("bing"), by="word") %>%
  count(document, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

Now we can compare the sentiments between documents, just as an example of what you can do with sentiment analysis. This can be done via ggplot2.

```{r}
p2 <- ggplot(multi_doc_sentiment, aes(document, sentiment, fill = document)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~document, ncol = 2, scales = "free_x")
```

```{r}
p2
```

As shown in these graphs, documents 4 and 5 are signficantly more negative in sentiment than the othe rthree documents.



